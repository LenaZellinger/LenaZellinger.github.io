---
# Display name
title: Lena Zellinger

# Name pronunciation (optional)
#name_pronunciation: Chien Shiung Wu

# Full name (for SEO)
first_name: Lena
last_name: Zellinger

# Is this the primary user of the site?
superuser: true

# Highlight the author in author lists? (true/false)
highlight_name: true

# Role/position/tagline
role: PhD student

# Organizations/Affiliations to display in Biography blox
organizations:
  - name: University of Edinburgh

# Social network links
# Need to use another icon? Simply download the SVG icon to your `assets/media/icons/` folder.
profiles:
  - icon: at-symbol
    url: 'mailto:L.Zellinger@sms.ed.ac.uk'
  - icon: brands/x
    url: https://bsky.app/profile/lenazellinger.bsky.social
  - icon: brands/github
    url: https://github.com/LenaZellinger
  - icon: academicons/google-scholar
    url: https://scholar.google.com/citations?user=JVsrfAYAAAAJ&hl=com

interests:
  - Tractable probabilistic modeling
  - Computational statistics
  - Uncertainty quantification
  - Neuro-symbolic AI

education:
  - area: PhD at the Institute for Adaptive and Neural Computation, current
    institution: University of Edinburgh
  - area: MSc in Data Science, 2022
    institution: University of Vienna
  - area: BSc in Statistics, 2020
    institution: University of Vienna
---
Hi! I am a first-year [ELLIS](https://ellis.eu/) PhD student at the University of Edinburgh, supervised by [Dr. Antonio Vergari](http://nolovedeeplearning.com/), [Dr. Nikolay Malkin](https://malkin1729.github.io/) and [Dr. Vincent Fortuin](https://fortuin.github.io/). I am part of the [APRIL lab](https://april-tools.github.io/) at the University of Edinburgh and the [ELPIS lab](https://fortuinlab.github.io/) at Helmholtz AI and TU Munich.

My main research interests include neuro-symbolic modeling and uncertainty quantification. In particular, I am eager to understand when and how we can satisfy constraints in our deep learning systems while providing well-calibrated predictions. At the same time, I aim to develop methods that are computationally efficient and can be scaled to complex problems. For instance, I have lately been working on [scalable expectation estimation with subtractive mixutre models](https://arxiv.org/abs/2503.21346).

If you are excited about similar topics, reach out - I am always happy to discuss!